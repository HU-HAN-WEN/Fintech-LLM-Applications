# 語言模型 - 生成品質評估 (LLM Generation Quality Evaluation)

## 專案目的
本專案旨在提供一個自動化評估框架，用於評估大型語言模型 (LLMs) 在不同任務（摘要、英翻中、中翻英）上的生成品質。透過結合 LLM (例如 GPT-4) 作為評審模型，以及 Gradio 建立的互動式介面，用戶可以方便地輸入文本、選擇生成模型，並獲得量化的評估分數與具體評語。

## 技術工具
* **程式語言:** Python
* **LLM 推理:** Hugging Face `transformers` (用於載入和生成模型)
* **LLM 評估:** OpenAI API (使用 GPT-4 作為評審模型)
* **非同步處理:** `asyncio`, `tqdm.asyncio` (提高 API 請求效率)
* **數據處理:** `json`, `datasets` (用於處理評估模板和結果)
* **使用者介面:** Gradio (快速建立互動式 Web UI)
* **環境管理:** `pip`, `requirements.txt`
* **版本控制:** Git / GitHub

## 技術架構與流程

本專案的核心流程如下圖所示：

```mermaid
graph TD
    A[用戶輸入文本] --> B{選擇生成模型};
    B --> C[生成模型生成回覆];
    C --> D{構造評估指令};
    D --> E[透過 OpenAI API 呼叫 GPT-4 評審模型];
    E --> F[評審模型輸出分數與評語];
    F --> G[解析評審結果];
    G --> H[計算總體分數並顯示];
    H --> I[Gradio 介面展示結果];


＃圖表說明：
#A：用戶輸入文本 - 用戶在 Gradio 介面中輸入待處理的原始文本。
#B：選擇生成模型 - 用戶從下拉列表中選擇預計用於生成摘要或翻譯的語言模型（例如TAIDE, Llama-3, Yi, Gemma）。這是一個決策點，影響後續的生成流程。(語言模型版本：TAIDE-LX-7B-Chat, Meta-Llama-3-8B-Instruct, Yi-1.5-9B-Chat, Gemma-7B)
#C：生成模型生成回覆 - 根據用戶輸入和選定的模型，程式會呼叫對應的語言模型 API（或使用預設範例回覆），生成摘要或翻譯結果。
#D：構造評估指令 - 將原始輸入、生成模型的回覆以及預設的評估指南（來自 JSON 模板）組合成一個標準化的提示，用於評審模型。
#E：透過 OpenAI API 呼叫 GPT-4 評審模型(TAIDE bench) - 構造好的評估指令被發送給 OpenAI 的 GPT-4 模型，作為自動評審員。
#F：評審模型輸出分數與評語 - GPT-4 根據其對指令的理解，對生成結果進行評估，並返回一個分數和詳細的評語。
#G：解析評審結果 - 程式解析 GPT-4 返回的文本，提取出量化的分數和完整的評語。
#H：計算總體分數並顯示 - 將解析出的分數進行計算（例如平均分），並準備在介面中顯示。
#I：Gradio 介面展示結果 - 最終，Gradio Web 介面會向用戶展示生成模型的原始回覆、GPT-4 給出的分數以及詳細評語。
