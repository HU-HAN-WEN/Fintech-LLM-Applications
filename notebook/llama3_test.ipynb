{"cells":[{"cell_type":"markdown","metadata":{"id":"RuSq7LecDGpg"},"source":["# Llama3 執行範例"]},{"cell_type":"markdown","metadata":{"id":"KwEivX8HCOT5"},"source":["進入筆記本後，如果要使用到GPU，請先設定\n","\n","設定GPU:\n","1. 編輯 -> 筆記本設定\n","2. 選擇L4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kgRfONrBqCu","outputId":"5ab4b621-20ca-4d90-c692-df5e53ff7652","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/297.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/297.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n"]}],"source":["!pip install accelerate"]},{"cell_type":"markdown","metadata":{"id":"4BYhnmCLC94X"},"source":["安裝完成後，點選執行階段->重新啟動工作階段"]},{"cell_type":"markdown","metadata":{"id":"56NJwVVSD9sL"},"source":["# 登入huggingface\n","\n","1. 輸入token\n","2. Add token as git credential? (Y/n) -> n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"iwMLHcOuD8cz","outputId":"ca525815-9428-4b42-ec8a-747471ca63bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: "]}],"source":["!huggingface-cli login"]},{"cell_type":"markdown","metadata":{"id":"plz0jnEIDPKJ"},"source":["# import packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWK2ta5DCNj2"},"outputs":[],"source":["import transformers\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"SsyFhTcrDUC5"},"source":["# load model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXwYGUQrBqHD"},"outputs":[],"source":["model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","\n","pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model_id,\n","    model_kwargs={\"torch_dtype\": torch.bfloat16},\n","    device_map=\"auto\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"XUzTBcMCDdU7"},"source":["# start to chat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gd6JEZIQBqTg"},"outputs":[],"source":["while True:\n","  user_input = input('請輸入你的問題：')\n","\n","  messages = [{\"role\": \"system\", \"content\": '你是一位很厲害且專業的助理，請回答我的問題'},\n","            {'role': \"user\", \"content\": user_input + '請用繁體中文回覆'}]\n","\n","\n","  prompt = pipeline.tokenizer.apply_chat_template(\n","  messages,\n","  tokenize=False,\n","  add_generation_prompt=True\n","  )\n","\n","  terminators = [\n","    pipeline.tokenizer.eos_token_id,\n","    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","  ]\n","\n","  outputs = pipeline(\n","    prompt,\n","    max_new_tokens=4096,\n","    eos_token_id=terminators,\n","    do_sample=True,\n","    temperature=0.6,\n","    top_p=0.9,\n","  )\n","  print(outputs[0][\"generated_text\"][len(prompt):])\n","  print('\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XpYS6-v4BqWB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaOfVtNsBqZK"},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}