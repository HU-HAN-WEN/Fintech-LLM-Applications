# -*- coding: utf-8 -*-
"""
èªè¨€æ¨¡å‹ - ç”Ÿæˆå“è³ªè©•ä¼°å·¥å…·

æ­¤è…³æœ¬æä¾›ä¸€å€‹åŸºæ–¼ Gradio çš„äº’å‹•å¼ä»‹é¢ï¼Œ
ç”¨æ–¼è©•ä¼°å¤§å‹èªè¨€æ¨¡å‹ (LLMs) åœ¨æ‘˜è¦ã€è‹±ç¿»ä¸­ã€ä¸­ç¿»è‹±ç­‰ä»»å‹™ä¸Šçš„ç”Ÿæˆå“è³ªã€‚
å®ƒåˆ©ç”¨ GPT-4 ä½œç‚ºè‡ªå‹•è©•å¯©ï¼Œå°æ¨¡å‹ç”Ÿæˆçš„å›ç­”é€²è¡Œåˆ†æ•¸è©•å®šå’Œæä¾›å…·é«”è©•èªã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- è¼‰å…¥ä¸¦å‘¼å«å¤§å‹èªè¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ Yi, TAIDE, Llama-3ï¼‰é€²è¡Œæ–‡æœ¬ç”Ÿæˆã€‚
- æ§‹é€ ç¬¦åˆè©•å¯©æ¨¡å‹ (GPT-4) è¦æ±‚çš„æç¤ºè©ã€‚
- é€é OpenAI API éåŒæ­¥å‘¼å« GPT-4 é€²è¡Œç”Ÿæˆå“è³ªè©•ä¼°ã€‚
- è§£æ GPT-4 çš„è©•å¯©çµæœï¼Œè¨ˆç®—åˆ†æ•¸ä¸¦æå–è©•èªã€‚
- é€é Gradio æä¾›ä½¿ç”¨è€…å‹å–„çš„ Web ä»‹é¢é€²è¡Œäº’å‹•å¼æ¸¬è©¦èˆ‡çµæœå±•ç¤ºã€‚

æ³¨æ„äº‹é …ï¼š
- æœ¬ç¨‹å¼ç¢¼ä¸­çš„æ¨¡å‹è¼‰å…¥è·¯å¾‘ (get_model å‡½æ•¸å…§) æ˜¯é‡å°ç‰¹å®šå…§éƒ¨ç’°å¢ƒè¨­ç½®ï¼Œ
  è‹¥è¦åœ¨å…¶ä»–ç’°å¢ƒé‹è¡Œç”Ÿæˆéƒ¨åˆ†ï¼Œè«‹è‡ªè¡Œèª¿æ•´æ¨¡å‹è¼‰å…¥é‚è¼¯æˆ–ç¢ºä¿æ¨¡å‹å¯ç”¨ã€‚
- è©•ä¼°åŠŸèƒ½éœ€è¦æœ‰æ•ˆçš„ OpenAI API Keyï¼Œè«‹åœ¨ç’°å¢ƒè®Šæ•¸ä¸­è¨­ç½® OPENAI_API_KEYã€‚
- Hugging Face æ¨¡å‹ç™»éŒ„å¯èƒ½éœ€è¦ HF_TOKEN ç’°å¢ƒè®Šæ•¸ã€‚

ä½œè€…ï¼šèƒ¡ç€šæ–‡ & Team (é»ƒç…’ç¿”, æèŠ·è‘³, å¾å£«ç¶­)
æ—¥æœŸï¼š2024å¹´6æœˆ30æ—¥
ç‰ˆæœ¬ï¼š1.0

"""

!pip install openai

!pip install gradio

!pip install datasets

import os
from openai import OpenAI
import httpx

import gradio as gr
import json
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset, Dataset

import asyncio
from tqdm.asyncio import tqdm as tqdm_async
from tqdm import tqdm, trange
from concurrent.futures import ProcessPoolExecutor
from openai import AsyncOpenAI, OpenAI
from huggingface_hub import login

# ç’°å¢ƒè¨­å®š
# è‹¥æ²’è¨­ï¼ŒH100æœƒæ“‹
# os.environ["no_proxy"] = "localhost, 127.0.0.1,::1"

from huggingface_hub import login
login() #Hugging Face çš„ç™»éŒ„ï¼Œé€™æœƒè¦æ±‚è¼¸å…¥token

async def get_completion(client, model, content):
  for _ in range(3):
      try:
          resp = await client.chat.completions.create(
              model=model,
              messages=content,
              temperature=0.2,
              timeout=180,
              seed=42
          )
          return parse_score(resp.choices[0].message.content)
      except Exception as e:
          # time.sleep(1)
          print(e)
          await asyncio.sleep(10)
  print('ignore', content)
  return {"score": -1, "judge_response": "error"}

async def get_completion_list(model, content_list, batch_size=20):
  client = AsyncOpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
  result = await tqdm_async.gather(*[get_completion(client, model, content) for content in content_list], )

  return result

def create_instruction(template: dict[str, str],
                       question: str,
                       responses: list[str],
                       ground_truth: list[str] = None):
    user_dct = {'question': question.strip()}
    for i, resp in enumerate(responses):
        user_dct[f'answer_{i+1}'] = resp.replace('\u200b', '')[:2000]

    # ground_truth
    user_dct['ground_truth'] = ground_truth
    user_context = template['user'].format(**user_dct)

    return [
        {"role": "system", "content": template["system"]},
        {"role": "user", "content": user_context}
    ]

def parse_score(review):
    try:
        score = review.split("\n")[-1].split(":")[-1]
        if '/10' in score:
            score = score.replace('/10', '')
        return {
            "score": eval(score),
            "judge_response": review,
        }
    except Exception as e:
        print(score)
        return {
            "score": 0,
            "judge_response": review,
        }

def compute_score(result, judge_resps):
  for prompt, resp in zip(result, judge_resps):
        prompt['judge_response'] = resp['judge_response']

        prompt['score'] = resp['score']

  # overall score
  overall_scores = 0
  result = [r for r in result if r['score'] != -1]
  for r in result:
      overall_scores += r['score']

  # save result
  with open('./hahah.json', 'w') as f:
    json.dump({"overall": {"score": overall_scores,
                "avg_score": overall_scores / len(result)},
          "result": result},
          f, indent=2, ensure_ascii=False)
  print(overall_scores / len(result))
  print(result[0]['judge_response'].split("\n\n")[0])
  return overall_scores / len(result), result[0]['judge_response'].split("\n\n")[0]


def evaluation(gen_dct, task):
  # è®€å–æ‘˜è¦è©•ä¼°prompt
  judge_path = {
    "summarization": "template_judge/geval_summarization.json",
    "translation": "template_judge/geval_translation.json",
    "zh2en": "template_judge/geval_translation_zh2en.json"
  }
  template_path = judge_path.get(task)
  print(template_path)
  #template_path = "template_judge/geval_summarization.json"
  with open(template_path) as f:
    template = json.load(f)
  # æ¨™æº–ç­”æ¡ˆ
  gt_dct = {#'resp': "äº‹å¯¦ä¸Šï¼Œé«˜æºªæ± åœ¨åŸºéš†å¸‚æ“”ä»»æ–°èæ¡è¨ªå·¥ä½œ48å¹´ï¼Œåœ¨æ–°èç•Œæ“æœ‰ã€Œè€å ±äººã€å°è™Ÿï¼Œæ›¾ä»»æ°‘çœ¾æ—¥å ±ã€é’å¹´æ—¥å ±ã€å¤§è¯æ™šå ±ã€æ­£è²é›»å°ã€å°ç£æ—¥å ±åŸºéš†ç‰¹æ´¾å“¡ï¼Œé‚„æ“”ä»»éåŸºéš†å¸‚å¤–å‹¤è¨˜è€…è¯èª¼æœƒç†äº‹é•·ã€‚ä¸åªæ–°èç•Œï¼Œé«˜æºªæ± åœ¨æ”¿å£‡ä¹Ÿæœ‰èˆ‰è¶³è¼•é‡çš„åœ°ä½ï¼Œä»–æ›¾å‡ºé8æœ¬ã€ŠåŸºéš†é¸èˆ‰éŒ„ã€‹ï¼Œä¸€è·¯å¾æœ€åŸºå±¤é‡Œé•·ã€è­°å“¡ã€å¸‚é•·ï¼Œåˆ°åœ‹å¤§ä»£è¡¨ã€ç«‹å§”ã€çœé•·ã€ç¸½çµ±å¤§é¸éƒ½å…¨è¨˜éŒ„ã€‚",
        'resp': "",
        'prompt': gen_dct['prompt'],
        'model': "grounf_truth",
        'qid': 0
  }
  # ground_truth and generation pairs
  resp_dct = {gt_dct['model']: gt_dct,
         gen_dct['model']: gen_dct
  }
  result = []
  resp_names = list(resp_dct.keys())
  question = resp_dct[resp_names[0]]['prompt'][0]['content']
  print("question:", question)
  model_resps = [resp_dct[name]['resp'] for name in resp_names]

  # å­˜æ”¾ground_truth and generation result
  result.append({
    "qid": 0,
    "question": gt_dct['prompt'],
    "model_responses": {name: resp_dct[name]['resp'] for name in resp_names},
    "eval_instruction": create_instruction(template, question, model_resps)
  })
  print(result[0]["model_responses"])
  print(result[0]["eval_instruction"])

  eval_instruction = [r.pop('eval_instruction') for r in result]
  judge_model = 'gpt-4'
  req_method = 'async'
  if req_method == 'async':
    judge_resps = asyncio.run(get_completion_list(
        judge_model, eval_instruction), debug=True)

  overall_scores, comments = compute_score(result, judge_resps)
  return overall_scores, comments


def get_model(model_name):
    """
    é€épipelineè¼‰å…¥æ¨¡å‹
    """
    model_type = {
        'yi-6B-chat': '../../../../../../raid2/model/models--01-ai--Yi-6B-Chat/snapshots/63d431abe178700a8c143a733e65caf6ed5a2614',
        'TAIDE-LX-7B-Chat': 'taide/TAIDE-LX-7B-Chat',
        'TAIDE_llama3_8B-chat': '../../../../../../raid2/model/TAIDE_llama3_8B-chat',
        'Meta-Llama-3-8B-Instruct': '../../../../../../raid2/model/Meta-Llama-3-8B-Instruct',
    }

    model_id = model_type.get(model_name)
    dtype = torch.bfloat16
    # load taide model
    pipe = pipeline("text-generation",
                    model=model_id,
                    model_kwargs={"torch_dtype": torch.bfloat16},
                    device_map="auto",)

    print('model loading done...')
    return pipe

def generate(prompt, pipe):
    """
    é€éè¼¸å…¥æ–‡ç« ç”¢ç”Ÿå°æ‡‰è¼¸å‡º
    """
    terminators = [pipe.tokenizer.eos_token_id,
                   pipe.tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]
    outputs = pipe(prompt,
                    max_new_tokens=2048,
                    eos_token_id=terminators,
                    #do_sample=True,
                    #temperature=0.6,
                    #top_p=0.9
    )
    return outputs[0]["generated_text"][len(prompt):]


def summary(text, model_name):
    """
    ç”¢ç”Ÿå‡ºæ‘˜è¦
    text : receive input text
    model_name : receive chosen model
    """
    task = "summarization"
    print(model_name)
    # è®€å–æ¨¡å‹
    #pipe = get_model(model_name)
    print("done...")

    # ä¸­è‹±æ–‡å›ç­”è¨­å®š

    if model_name == "Meta-Llama-3-8B-Instruct":
      message = f"""<s>[INST]å¹«æˆ‘æ‘˜è¦ï¼Œä¸¦ä»¥ç¹é«”ä¸­æ–‡å›ç­”\n\n{text} [/INST]"""
    else:
      message = f"""<s>[INST]å¹«æˆ‘æ‘˜è¦\n\n{text} [/INST]"""

    message = [{'role': "user", "content": message}]
    question = {'resp': '', 'prompt': message, 'model': model_name, 'qid': 0}
    """
    prompt = pipe.tokenizer.apply_chat_template(question['prompt'],
                            tokenize=False,
                            add_generation_prompt=True
    )
    # ç”¢ç”Ÿå›è¦†
    resopnse = generate(prompt, pipe)
    """
    resopnse = "ä¸€åç¶²å‹åœ¨æ¯è¦ªç½¹æ‚£æ–°å† è‚ºç‚å¾Œä½µç™¼ç—‡åŠç³–å°¿ç—…å°è‡´å¥åº·æƒ¡åŒ–ï¼Œä¸¦åœ¨ç—…åºŠä¸Šèˆ‡æ¯è¦ªæ‹ä¸‹æœ€å¾Œä¸€å¼µåˆç…§ï¼Œè¡¨é”å°æ¯è¦ªçš„æ·±æƒ…é“åˆ¥ã€‚åŸPOå›æ†¶æ¯è¦ªä¸€ç”Ÿçš„é—œæ„›èˆ‡ä»˜å‡ºï¼Œæ„Ÿå˜†å­©å­é•·å¤§å¾Œå¾€å¾€è¦é¢å°çš„æ˜¯çˆ¶æ¯å¥åº·çš„æƒ¡åŒ–ã€‚æ–‡ç« å……æ»¿æ„Ÿå‚·ï¼Œå¼•èµ·ç¶²å‹å…±é³´ï¼Œæ·šå´©ä¹‹é¤˜ä¹Ÿæ›´åŠ çæƒœèˆ‡å®¶äººç›¸è™•çš„æ™‚å…‰ã€‚\nåŸPOæ•˜è¿°å…¶æ¯è¦ªåœ¨å»å¹´æŸ“ç–«ï¼Œä¹‹å¾Œå¯èƒ½å› æ–°å† å¾Œéºç—‡åŠç³–å°¿ç—…å•é¡Œï¼Œå¿ƒè¡€ç®¡èˆ‡è…è‡ŸåŠŸèƒ½å‡ºç¾å•é¡Œï¼Œé†«ç”Ÿæé†’å®¶å±¬è¦å¤šåŠ çœ‹ç®¡ã€‚ä¹‹å¾Œï¼Œæ¯è¦ªç™¼ç”Ÿè¡€ç³–éä½æ˜è¿·çš„äº‹ä»¶ï¼ŒåŸPOé›–æé†’æ¯è¦ªè‹¥æœ‰ä¸é©è¦è¯ç¹«ï¼Œä½†éš¨å¾Œå»å†ä¹Ÿæ²’æœ‰é†’éä¾†ï¼Œæœ€çµ‚åœ¨æŠµé”é†«é™¢å‰ä¸å¹¸éä¸–ã€‚"
    # åŠ å…¥å­—å…¸
    gen_dct = {'resp': resopnse,
          'prompt': question['prompt'],
          'model': model_name,
          'qid': question['qid']
    }
    print("gen_dct:", gen_dct)
    overall_scores, comments = evaluation(gen_dct, task)
    overall_scores = f"{overall_scores}/10"

    return gen_dct['resp'], overall_scores, comments

# è‹±ç¿»ä¸­
def translation(text, model_name):
  task = "translation"
  print(model_name)
  # è®€å–æ¨¡å‹
  #pipe = get_model(model_name)
  print("done...")

  # promptè¨­å®š
  message = f"""<s>[INST]ä»¥ä¸‹æä¾›è‹±æ–‡æ–‡ç« ï¼Œè«‹å¹«æˆ‘ç¿»è­¯æˆç¹é«”ä¸­æ–‡\n\n{text} [/INST]"""
  message = [{'role': "user", "content": message}]
  question = {'resp': '', 'prompt': message, 'model': model_name, 'qid': 0}
  """
  prompt = pipe.tokenizer.apply_chat_template(question['prompt'],
                          tokenize=False,
                          add_generation_prompt=True
  )
  # ç”¢ç”Ÿå›è¦†
  resopnse = generate(prompt, pipe)
  """
  resopnse = "å†¬å±±å’–å•¡ä»¥å¾—å¤©ç¨åšçš„ä½ç½®ï¼Œä¸æ–·ç£¨ç·´å‡ºåŠ å·¥æ³•çš„ç²¾é€²ï¼Œé¢¨å‘³æ·±ç²çœ¾å¤šå’–å•¡è¿·çš„æ¨å´‡ã€‚"

  # åŠ å…¥å­—å…¸
  gen_dct = {'resp': resopnse,
        'prompt': question['prompt'],
        'model': model_name,
        'qid': question['qid']
  }
  print("gen_dct:", gen_dct)
  overall_scores, comments = evaluation(gen_dct, task)
  overall_scores = f"{overall_scores}/10"

  return gen_dct['resp'], overall_scores, comments

# ä¸­ç¿»è‹±
def translation_zh2en(text, model_name):
  task = "zh2en"
  print(model_name)
  # è®€å–æ¨¡å‹
  #pipe = get_model(model_name)
  print("done...")

  # promptè¨­å®š
  message = f"""<s>[INST]ä»¥ä¸‹æ˜¯ä¸€ç¯‡ä¸­æ–‡å…§å®¹ï¼Œè«‹å¹«æˆ‘ç¿»è­¯æˆè‹±æ–‡\n\n{text} [/INST]"""
  message = [{'role': "user", "content": message}]
  question = {'resp': '', 'prompt': message, 'model': model_name, 'qid': 0}
  """
  prompt = pipe.tokenizer.apply_chat_template(question['prompt'],
                          tokenize=False,
                          add_generation_prompt=True
  )
  # ç”¢ç”Ÿå›è¦†
  resopnse = generate(prompt, pipe)
  """
  resopnse = "The pork slices are laid out in circle after circle, with another circle placed in the middle. Paired with the visual effect of dry ice, it looks like a volcano eruption."

  # åŠ å…¥å­—å…¸
  gen_dct = {'resp': resopnse,
        'prompt': question['prompt'],
        'model': model_name,
        'qid': question['qid']
  }
  print("gen_dct:", gen_dct)
  overall_scores, comments = evaluation(gen_dct, task)
  overall_scores = f"{overall_scores}/10"

  return gen_dct['resp'], overall_scores, comments

def clear_data():
  return "", "", "", "", ""



if __name__ =="__main__":
  with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# TAIDE BENCH EVALUATIONğŸ˜Š")
    # é€™è£¡æ˜¯æ‘˜è¦...
    with gr.Tab("æ‘˜è¦"):
      gr.Markdown("""## é€éæ¬²ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ‘˜è¦å¾Œï¼Œå†é€éGPT4é€²è¡Œè©•ä¼°\n
      è©•ä¼°æŒ‡å—ï¼š
      - ç°¡å–®æ˜ç­ï¼šæª¢æŸ¥æ˜¯å¦ç°¡å–®æ˜ç­çš„ä¿ç•™åŸå§‹æ–‡ç« å¤§è‡´å…§å®¹ï¼Œé¿å…é™·å…¥ä¸é‡è¦çš„ç´°ç¯€ã€‚
      - ç”¨è©é¸æ“‡ï¼šæª¢æŸ¥ä½¿ç”¨çš„è©å½™æ˜¯å¦ç¬¦åˆå°ç£ä¸­æ–‡çš„ç¿’æ…£ï¼Œä¸”è©²ä½¿ç”¨åŸæ–‡æ™‚ä¿ç•™åŸå§‹èªè¨€ã€‚
      """)
      chosen_model1 = gr.Dropdown(["TAIDE-LX-7B-Chat", "TAIDE_llama3_8B-chat", "Meta-Llama-3-8B-Instruct", "Yi-1.5-6B-Chat", "mistralai/Mistral-7B-Instruct-v0.2"], label="æ¬²ä½¿ç”¨æ¨¡å‹")
      text_input1 = gr.Textbox(lines=3, placeholder="è«‹è¼¸å…¥æ–‡ç« ", label="Context")
      with gr.Row():
        text_button1 = gr.Button("Generate")
        clear_button1 = gr.Button("Clear")
      text_output1 = gr.Textbox(lines=3, label="Summary")
      gr.Markdown("""
      åˆ†æ•¸èªªæ˜ï¼š
      - 1-3åˆ†ï¼šæ‘˜è¦æ˜é¡¯æœ‰èª¤æˆ–å›ç­”åŸæ–‡ã€‚
      - 4-6åˆ†ï¼šæ‘˜è¦å­˜åœ¨ä¸€äº›æ˜é¡¯çš„éŒ¯èª¤æˆ–éºæ¼ã€‚
      - 7-8åˆ†ï¼šæ‘˜è¦å¤§è‡´ä¸Šæ˜¯æ­£ç¢ºçš„ï¼Œä½†é‚„æœ‰ä¸€äº›å°å•é¡Œã€‚
      - 9-10åˆ†ï¼šæ‘˜è¦éå¸¸ç²¾ç¢ºï¼Œå¹¾ä¹æ²’æœ‰ä»»ä½•å•é¡Œã€‚
      """)
      score_optput1 = gr.Textbox(label="Score")
      comment_output1 = gr.Textbox(label="è©•èª")

    # é€™è£¡æ˜¯è‹±ç¿»ä¸­...
    with gr.Tab("è‹±ç¿»ä¸­"):
      gr.Markdown("""## é€éæ¬²ä½¿ç”¨æ¨¡å‹ç”Ÿæˆç¿»è­¯å¾Œï¼Œå†é€éGPT4é€²è¡Œè©•ä¼°\n
      è©•ä¼°æŒ‡å—ï¼š
      - èªæ³•æ­£ç¢ºæ€§ï¼šæª¢æŸ¥ç¿»è­¯çš„å¥å­æ˜¯å¦åœ¨èªæ³•ä¸Šæ­£ç¢ºç„¡èª¤ã€‚
      - ç”¨è©é¸æ“‡ï¼šæª¢æŸ¥ä½¿ç”¨çš„è©å½™æ˜¯å¦æ­£ç¢ºä¸”é©ç•¶ï¼Œä¸¦ç¬¦åˆå°ç£ä¸­æ–‡çš„ç¿’æ…£ã€‚
      - ä¿ç•™åŸæ–‡æ„æ€ï¼šç¿»è­¯æ˜¯å¦å¿ å¯¦æ–¼åŸæ–‡ï¼Œä¸¦ä¿ç•™å…¶ä¸»è¦æ„æ€å’Œç´°ç¯€ã€‚
      - æ–‡åŒ–å’Œèªå¢ƒé©æ‡‰æ€§ï¼šæª¢æŸ¥ç¿»è­¯æ˜¯å¦è€ƒæ…®åˆ°å°ç£çš„æ–‡åŒ–å’Œèªå¢ƒï¼Œç‰¹åˆ¥æ˜¯ç•¶åŸæ–‡ä¸­æœ‰å¯èƒ½ç”¢ç”Ÿèª¤è§£æˆ–èˆ‡å°ç£æ–‡åŒ–æœ‰å‡ºå…¥çš„å…§å®¹ã€‚
      - ä½¿ç”¨ç›®æ¨™èªè¨€ï¼šæª¢æŸ¥æ˜¯å¦å®Œå…¨ä½¿ç”¨äº†ç›®æ¨™èªè¨€ï¼Œä¸¦é¿å…äº†ä¸å¿…è¦çš„åŸæ–‡èªè¨€å…§å®¹ã€‚
      """)
      chosen_model2 = gr.Dropdown(["TAIDE-LX-7B-Chat", "TAIDE_llama3_8B-chat", "Meta-Llama-3-8B-Instruct", "Yi-1.5-6B-Chat", "mistralai/Mistral-7B-Instruct-v0.2"], label="æ¬²ä½¿ç”¨æ¨¡å‹")
      text_input2 = gr.Textbox(lines=3, placeholder="è«‹è¼¸å…¥æ–‡ç« ", label="Context")
      with gr.Row():
        text_button2 = gr.Button("Generate")
        clear_button2 = gr.Button("Clear")
      text_output2 = gr.Textbox(lines=3, label="Translation")
      gr.Markdown("""
      åˆ†æ•¸èªªæ˜ï¼š
      - 1-3åˆ†ï¼šå¤§éƒ¨åˆ†çš„ç¿»è­¯éƒ½å­˜åœ¨å•é¡Œã€‚
      - 4-6åˆ†ï¼šç¿»è­¯ä¸­å­˜åœ¨ä¸€äº›æ˜é¡¯çš„éŒ¯èª¤æˆ–éºæ¼ã€‚
      - 7-8åˆ†ï¼šç¿»è­¯å¤§è‡´ä¸Šæ˜¯æ­£ç¢ºçš„ï¼Œä½†é‚„æœ‰ä¸€äº›å°å•é¡Œã€‚
      - 9-10åˆ†ï¼šç¿»è­¯éå¸¸ç²¾ç¢ºï¼Œå¹¾ä¹æ²’æœ‰ä»»ä½•å•é¡Œã€‚
      """)
      score_optput2 = gr.Textbox(label="Score")
      comment_output2 = gr.Textbox(label="è©•èª")

    # é€™è£¡æ˜¯ç¿»è­¯...
    with gr.Tab("ä¸­ç¿»è‹±"):
      gr.Markdown("""## é€éæ¬²ä½¿ç”¨æ¨¡å‹ç”Ÿæˆç¿»è­¯å¾Œï¼Œå†é€éGPT4é€²è¡Œè©•ä¼°\n
      è©•ä¼°æŒ‡å—ï¼š
      - èªæ³•æ­£ç¢ºæ€§ï¼šæª¢æŸ¥ç¿»è­¯çš„å¥å­æ˜¯å¦åœ¨èªæ³•ä¸Šæ­£ç¢ºç„¡èª¤ã€‚
      - ç”¨è©é¸æ“‡ï¼šæª¢æŸ¥ä½¿ç”¨çš„è©å½™æ˜¯å¦æ­£ç¢ºä¸”é©ç•¶ï¼Œä¸¦ç¬¦åˆå°ç£çš„ç¿’æ…£ã€‚
      - ä¿ç•™åŸæ–‡æ„æ€ï¼šç¿»è­¯æ˜¯å¦å¿ å¯¦æ–¼åŸæ–‡ï¼Œä¸¦ä¿ç•™å…¶ä¸»è¦æ„æ€å’Œç´°ç¯€ã€‚
      - æ–‡åŒ–å’Œèªå¢ƒé©æ‡‰æ€§ï¼šæª¢æŸ¥ç¿»è­¯æ˜¯å¦è€ƒæ…®åˆ°å°ç£çš„æ–‡åŒ–å’Œèªå¢ƒï¼Œç‰¹åˆ¥æ˜¯ç•¶åŸæ–‡ä¸­æœ‰å¯èƒ½ç”¢ç”Ÿèª¤è§£æˆ–èˆ‡å°ç£æ–‡åŒ–æœ‰å‡ºå…¥çš„å…§å®¹ã€‚
      - ä½¿ç”¨ç›®æ¨™èªè¨€ï¼šæª¢æŸ¥æ˜¯å¦å®Œå…¨ä½¿ç”¨äº†ç›®æ¨™èªè¨€ï¼Œä¸¦é¿å…äº†ä¸å¿…è¦çš„åŸæ–‡èªè¨€å…§å®¹ã€‚
      """)
      chosen_model3 = gr.Dropdown(["TAIDE-LX-7B-Chat", "TAIDE_llama3_8B-chat", "Meta-Llama-3-8B-Instruct", "Yi-1.5-6B-Chat", "mistralai/Mistral-7B-Instruct-v0.2"], label="æ¬²ä½¿ç”¨æ¨¡å‹")
      text_input3 = gr.Textbox(lines=3, placeholder="è«‹è¼¸å…¥æ–‡ç« ", label="Context")
      with gr.Row():
        text_button3 = gr.Button("Generate")
        clear_button3 = gr.Button("Clear")
      text_output3 = gr.Textbox(lines=3, label="Translation")
      gr.Markdown("""
      åˆ†æ•¸èªªæ˜ï¼š
      - 1-3åˆ†ï¼šå¤§éƒ¨åˆ†çš„ç¿»è­¯éƒ½å­˜åœ¨å•é¡Œã€‚
      - 4-6åˆ†ï¼šç¿»è­¯ä¸­å­˜åœ¨ä¸€äº›æ˜é¡¯çš„éŒ¯èª¤æˆ–éºæ¼ã€‚
      - 7-8åˆ†ï¼šç¿»è­¯å¤§è‡´ä¸Šæ˜¯æ­£ç¢ºçš„ï¼Œä½†é‚„æœ‰ä¸€äº›å°å•é¡Œã€‚
      - 9-10åˆ†ï¼šç¿»è­¯éå¸¸ç²¾ç¢ºï¼Œå¹¾ä¹æ²’æœ‰ä»»ä½•å•é¡Œã€‚
      """)
      score_optput3 = gr.Textbox(label="Score")
      comment_output3 = gr.Textbox(label="è©•èª")

    # æ‘˜è¦
    text_button1.click(fn=summary, inputs=[text_input1, chosen_model1], outputs=[text_output1, score_optput1, comment_output1])
    clear_button1.click(fn=clear_data, inputs=[], outputs=[text_input1, chosen_model1, text_output1, score_optput1, comment_output1])
    #è‹±ç¿»ä¸­
    text_button2.click(translation, inputs=[text_input2, chosen_model2], outputs=[text_output2, score_optput2, comment_output2])
    clear_button2.click(fn=clear_data, inputs=[], outputs=[text_input2, chosen_model2, text_output2, score_optput2, comment_output2])
    #ä¸­ç¿»è‹±
    text_button3.click(translation_zh2en, inputs=[text_input3, chosen_model3], outputs=[text_output3, score_optput3, comment_output3])
    clear_button3.click(fn=clear_data, inputs=[], outputs=[text_input3, chosen_model3, text_output3, score_optput3, comment_output3])

  #demo.launch(ssl_verify=False)
  demo.launch(share=True)

